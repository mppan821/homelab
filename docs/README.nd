# üè° Homelab Kubernetes Foundation Plan (kubeadm with Namespaces)

## üéØ Goal

Build a **production-style Kubernetes homelab** on Proxmox (1 control plane, 2 worker nodes) using **kubeadm**.
Use **namespaces** (`staging` and `production`) for environment separation.

This approach keeps resource usage modest while still modelling:

* Shared infrastructure with clear environment boundaries.
* The ability to hibernate staging workloads by scaling replicas to zero.
* FluxCD already governs the platform add-ons and manages the sample nginx app as a reference implementation; this plan focuses on extending GitOps to additional application overlays and release workflows.
* If Flux reports missing CRDs for a HelmRelease (e.g., cert-manager or MetalLB), install the upstream CRDs once and re-run `flux reconcile kustomization homelab --with-source`.
* Optionally deploy the Weave GitOps UI by following `docs/weave-gitops-ui.md` for a read-only dashboard.

---

## üì¶ Core Components & Install Order

1. **Networking & Metrics**
   * Cilium (CNI replacing kube-proxy‚Äôs default routing).
   * metrics-server for HPA.
   * kube-state-metrics + node-exporter for cluster insights.

2. **Ingress & Certificates**
   * NGINX Ingress Controller.
   * Cert-Manager (Let‚Äôs Encrypt automation).
   * Separate domains/paths for staging vs production, e.g. `immich.lab.example.com` and `staging.immich.lab.example.com`.

3. **Storage**
   * Longhorn (SSD for performance workloads, HDD for bulk storage) managed via Flux.

4. **Observability**
   * Prometheus + Grafana.
   * Loki + Promtail (logs).
   * Optional Alertmanager.

5. **Secrets & Configs**
   * Sealed Secrets so encrypted secrets can live in Git.

6. **Backup & DR**
   * Velero backing namespaces and PVCs to MinIO, S3, or local storage.

7. **GitOps**
   * FluxCD Kustomizations per namespace (`immich-staging`, `immich-prod`) to drive continuous delivery. (Argo CD remains an alternative if requirements change.)

---

## üöÄ App Deployments (Namespace Approach)

* **Immich**
  * Prod: `production/immich` (bound to HDD Longhorn storage).
  * Staging: `staging/immich` (scaled to zero until testing).

* **Joplin Server**
  * Prod: `production/joplin`.
  * Staging: `staging/joplin` (smaller DB requests/limits).

* **Karakeep**
  * Prod: `production/karakeep`.
  * Staging: `staging/karakeep`.

---

## üõ†Ô∏è Workflow for Staging Apps

1. Model each workload with a shared `base/` (Deployments, Services, etc.) plus `overlays/` per environment so hostname, scaling, and secrets diverge cleanly‚Äîsee `apps/sample-nginx/` for an example.
2. Deploy manifests to **both namespaces** (`production` and `staging`).
3. Keep staging replicas at `0` by default:

   ```yaml
   replicas: 0
   ```
4. When testing, scale up:

   ```bash
   kubectl -n staging scale deploy immich --replicas=1
   ```
5. After testing, scale back down:

   ```bash
   kubectl -n staging scale deploy immich --replicas=0
   ```

---

## üìÖ Milestones

1. **MVP 1 ‚Äì Cluster Baseline**
   * kubeadm + containerd + Cilium + metrics stack.

2. **MVP 2 ‚Äì External Access**
   * NGINX Ingress + Cert-Manager + TLS test app.

3. **MVP 3 ‚Äì Storage Ready**
   * Longhorn with SSD/HDD separation.

4. **MVP 4 ‚Äì Observability**
   * Prometheus + Grafana dashboards, Loki logs.

5. **MVP 5 ‚Äì Secrets & Backups**
   * Sealed Secrets + Velero backup/restore.

6. **MVP 6 ‚Äì GitOps with Namespaces**
   * FluxCD syncing staging & production overlays with environment-specific policies.
