## Troubleshooting and Notes

- **SSH failures:** Verify `ssh_private_key_path` points to the correct private key and that Proxmox firewall rules allow SSH.
- **kubeadm init issues:** Check `sudo journalctl -u kubelet -f` on `master-node` and confirm containerd is healthy (`sudo systemctl status containerd`).
- **Nodes not joining:** Ensure the workers can reach `https://192.168.0.100:6443` and that `kubeadm token create --print-join-command` returns a valid token.
- **Worker nodes still missing:** Confirm the join command is executed on each worker (Terraform does this automatically). If you taint the worker null resource and re-apply, run `ssh -i <key> ubuntu@<worker> sudo journalctl -u kubelet -f` to watch for TLS/bootstrap failures.
- **Join command truncated:** If kubelet logs say `/var/lib/kubelet/config.yaml` is missing and `JOIN_CMD=kubeadm` appears in Terraform output, rerun with the latest code that base64-encodes the join command (post-`2025-10-05`). Before reapplying, `sudo kubeadm reset -f` on the worker, clear `/etc/kubernetes` and `/var/lib/kubelet`, then `terraform -chdir=infrastructure/terraform taint 'null_resource.worker_install["worker-node-1"]'` (and others) followed by `terraform apply`.
- **Terraform using wrong SSH key:** If Terraform logs warn that `../../id_ed25519` is missing, inspect the active value with `terraform -chdir=infrastructure/terraform console` â†’ `local.ssh_key_path` / `var.ssh_private_key_path`. Fix the path in `terraform.tfvars` (or place the key where Terraform expects it) before reapplying.
- **Re-running worker provisioning:** Use single quotes when tainting indexed resources (e.g. `terraform -chdir=infrastructure/terraform taint 'null_resource.worker_install["worker-node-1"]'`) and then `terraform -chdir=infrastructure/terraform apply`.
- **Proxmox VM storage not present:** Verify the additional data disk exists in Proxmox and is backed by a storage pool. If the `hdd1` directory does not show up under `pvesm status`, add it with `pvesm add dir hdd1-storage --path /mnt/hdd1 --content images,rootdir,iso,vztmpl,backup,snippets --mkdir 0` before creating the VMs.
- **MetalLB services not reachable:** Confirm the IP range defined in `../clusters/homelab/infrastructure/addons/metallb/addresspool.yaml` stays inside the same LAN as your nodes (e.g., `192.168.0.200-210` if the cluster sits on `192.168.0.0/24`). Addresses outside the local subnet will not route over Wi-Fi/LAN clients.
- **Cert-manager stuck on challenges:** Make sure the `letsencrypt-cloudflare` `ClusterIssuer` shows `Ready=True` (`kubectl describe clusterissuer letsencrypt-cloudflare`) and that wildcard/host-specific challenges in `kubectl get challenges -A` report `Valid`. If they remain `Pending`, verify the Cloudflare token and DNS propagation.
- **Flux reports missing CRDs:** The repository reconciles cert-manager (`v1.18.2`) and MetalLB (`v0.15.2`) CRDs automatically. If reconciliation still fails (for example, due to blocked network access), re-run `kubectl apply --server-side -f https://github.com/cert-manager/cert-manager/releases/download/v1.18.2/cert-manager.crds.yaml` and `kubectl apply --server-side -k "github.com/metallb/metallb/config/crd?ref=v0.15.2"`, then reconcile Flux again.
- **Flux cannot read OCI HelmRepository:** The Flux `source-controller` needs the `--feature-gates=OCIRepositories=true` flag to talk to OCI registries such as `oci://ghcr.io/weaveworks/charts`. The Flux kustomization in `clusters/homelab/flux-system/kustomization.yaml` patches the deployment to add that flag. If you see `HelmRepository/... NotReady` or `no matches for kind "HelmRelease" in version "helm.toolkit.fluxcd.io/v2beta1"`, make sure `kubectl apply -k clusters/homelab/flux-system` has been run after pulling the latest manifests so the CRDs and controller patch are installed.
- **HelmRelease fails with Invalid chart reference:** Confirm the chart/version exists in the referenced HelmRepository. If not, update the repo or remove the release (e.g., we removed local-path-provisioner in favor of Longhorn).
- **ExternalDNS not updating records:** Check the ExternalDNS deployment logs (`kubectl logs -n external-dns deploy/external-dns`) and confirm the Cloudflare API token secret matches the values in `../clusters/homelab/infrastructure/addons/external-dns/helmrelease.yaml`. Successful syncs appear as `Applied desired changes` entries.
- **Why the CRDs reconcile first?** Cert-manager and MetalLB publish their CustomResourceDefinitions outside their Helm charts. The dedicated `infrastructure-crds` Flux Kustomization installs them before the Helm releases in `infrastructure-addons` render their custom resources, which keeps dry-run validation from failing.
- Update the hostnames in `../apps/sample-nginx/overlays/{staging,production}/domain.env`, commit, and watch `flux get kustomizations` until `sample-nginx-staging` and `sample-nginx-production` report `Ready=True`. Use `kubectl get svc -n staging sample-nginx` (or `production`) to confirm service exposure once reconciling is complete.
- Review the GitOps definitions in `../clusters/homelab/infrastructure/` (Cilium, cert-manager, ExternalDNS, MetalLB, Longhorn, metrics-server) and adjust chart values as your environment evolves.
- Once storage reconciles, confirm `kubectl get sc` shows `longhorn` marked as `(default)` and run `kubectl -n longhorn-system get pods` to ensure the data plane is healthy.
- Default UI credentials live in the Weave GitOps HelmRelease (`adminUser.create: true`). Update the `passwordHash` and reconcile Flux to rotate the dashboard password.
- Layer on the observability stack and additional services following the roadmap in `docs/architecture.md` and the ADRs, committing the manifests under `clusters/homelab` and introducing new Flux Kustomizations as needed so dependencies stay explicit.
- Commit any environment-specific overrides (without secrets) to keep the bootstrap repeatable.
- Tune `vm_configs` or `kubeadm_install_revision` to change resources, IPs, or force reconfiguration.
